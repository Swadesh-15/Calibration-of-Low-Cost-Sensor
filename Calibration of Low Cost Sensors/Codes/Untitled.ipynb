{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fa9c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### supporting functions to machine_learning_calibration_and_site_transfer_test.ipynb\n",
    "### for outlier removal, data loading, reading raw data, \n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import r2_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor as GPR\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel as WK,\\\n",
    "ExpSineSquared as ESS, RationalQuadratic as RQ, Matern as M, DotProduct\n",
    "import scipy.stats\n",
    "import scipy.optimize as spo\n",
    "import scipy.stats as scs\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def read_raw(filename,start_experiment, end_experiment, colocated_devices):\n",
    "    # Read in file\n",
    "    \n",
    "    df = pd.read_csv(filename,  #dtype={\"pm10\": np.float64, \"pm2_5\": np.float64}, \n",
    "                     na_values=['Infinity'],\n",
    "                     parse_dates=['timestamp']\n",
    "                    )\n",
    "    # filter by date and device\n",
    "    \n",
    "    df = df[(df.timestamp >= start_experiment) & (df.timestamp <= end_experiment) & (df.id.apply(lambda x : x in colocated_devices))]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_kings(path, start_experiment, end_experiment):\n",
    "    df_kings_vol = pd.read_csv(path)\n",
    "    df_kings_vol['timestamp'] = pd.to_datetime(df_kings_vol.date)\n",
    "    # co is in ppm\n",
    "    if 'co' in df_kings_vol.columns:\n",
    "        df_kings_vol.co = 1000 * df_kings_vol.co\n",
    "\n",
    "    df_kings_vol = df_kings_vol[ (df_kings_vol.timestamp > start_experiment) & (df_kings_vol.timestamp < end_experiment)]\n",
    "    df_kings_vol = df_kings_vol.set_index('timestamp')\n",
    "    ds_kings_vol = xr.Dataset.from_dataframe(df_kings_vol)\n",
    "    \n",
    "    return ds_kings_vol\n",
    "\n",
    "# def find_start_end_coloc(df_coloc_times, sensor_id, ):\n",
    "#     start = df_coloc_times[df_coloc_times.id == sensor_id]['good period of colocation at CR7: Start'].values[0]\n",
    "#     end = df_coloc_times[df_coloc_times.id == sensor_id]['good period of colocation at CR7: End'].values[0]\n",
    "#     return start, end\n",
    "\n",
    "\n",
    "def preprocess(df, start_experiment):\n",
    "    reading_time = df['timestamp']\n",
    "\n",
    "    # Get hour, date, dayofweek\n",
    "    df['hr'] = reading_time.apply(lambda x: x.hour)\n",
    "    df['date'] = reading_time.apply(lambda x: x.date())\n",
    "    df['dayofweek'] = reading_time.apply(lambda x: x.dayofweek + 1)  # Monday=1, Sunday=7\n",
    "    df['month'] = reading_time.apply(lambda x: x.month)\n",
    "    df['week'] = reading_time.apply(lambda x: x.week)\n",
    "    df['count'] = 1\n",
    "\n",
    "    df['time_since_start'] = reading_time.apply(lambda x: x - start_experiment)\n",
    "    since_start = df['time_since_start']\n",
    "    df['days_since_start'] = since_start.apply(lambda x: x.days)\n",
    "\n",
    "    df['weeks_since_start'] = df['days_since_start'].apply(lambda x: int(x / 7))\n",
    "    df.drop('time_since_start', axis=1)\n",
    "\n",
    "    df = df.sort_values(by='timestamp')\n",
    "    return df\n",
    "\n",
    "### MAD outlier removal\n",
    "\n",
    "def remove_outliers_PM10(df, thresh=3.5):\n",
    "    def mad_based_outlier(points, thresh=3.5):\n",
    "        if len(points.shape) == 1:\n",
    "            points = points[:, None]\n",
    "        median = np.median(points, axis=0)\n",
    "        diff = np.sum((points - median) ** 2, axis=-1)\n",
    "        diff = np.sqrt(diff)\n",
    "        med_abs_deviation = np.median(diff)\n",
    "\n",
    "        modified_z_score = 0.6745 * diff / med_abs_deviation\n",
    "\n",
    "        return modified_z_score > thresh\n",
    "\n",
    "    limits_dict = {}\n",
    "\n",
    "    df_numeric = df.select_dtypes(include=[np.number])\n",
    "    # cols = set(df_numeric.columns)\n",
    "    # cols = cols.difference(set(['time', 'lat', 'long', 'fix', 'speed', 'alt', 'head', 'rpm', 'id', 'ver',\n",
    "    #                             'index', 'vol', 'rtys']))\n",
    "    cols = ['pm2hum', 'pm225a', 'pm2par25', 'pm1par5', 'pm2tmp', 'pm125a', 'pm210a', 'pm2par5', 'pm2par3',\\\n",
    "                                'pm125c', 'pm225c', 'pm1par10', 'pm11a', 'pm1par3', 'pm21c', 'pm21a',\\\n",
    "                                'pm110a', 'pm11c', 'pm1par25', 'pm210c', 'pm1tmp', 'pm110c', 'pm2par10', 'pm1hum']\n",
    "    print(cols)\n",
    "    df_no_na = df.dropna(subset=cols)\n",
    "    print(df_no_na.shape,'df_no_na.shape')\n",
    "    id_list = df_no_na.id.unique()\n",
    "    id_list.sort()\n",
    "    for col in cols:\n",
    "        for device_id in id_list:\n",
    "#             print(df_no_na[df_no_na.id == device_id].shape)\n",
    "            df_device = df_no_na[df_no_na.id == device_id]\n",
    "            data_col_id = df_device[col].values\n",
    "            outliers = mad_based_outlier(data_col_id, thresh=thresh)\n",
    "#             print(outliers.shape)\n",
    "            n_entries = len(data_col_id)\n",
    "            perc_outliers = sum(outliers) / n_entries\n",
    "            if perc_outliers > 0.05:\n",
    "                print(\"Column: %s Device id %i Number of entries %i Share of outliers %f\" % (\n",
    "                    col, device_id, n_entries, perc_outliers))\n",
    "#             print(df_device.index.shape,'index.shape')\n",
    "#             print(outliers.shape,'outliers shape')\n",
    "#             print(df_no_na.shape,'df_no_na.shape')\n",
    "            df_no_na.loc[df_device.index, col + '_outlier'] = outliers\n",
    "#     print(df_no_na.columns)\n",
    "    df_clean = df_no_na.copy()\n",
    "    for col in cols:\n",
    "        outlier_col = col + '_outlier'\n",
    "        df_clean = df_clean[~df_clean[outlier_col]]\n",
    "    print(\"Total Number of entries %i Perc remaining: %f\" % (len(df_no_na), len(df_clean) / len(df_no_na)))\n",
    "    return df_clean[df.columns], limits_dict\n",
    "\n",
    "def remove_outliers_NO2(df, thresh=3.5):\n",
    "    def mad_based_outlier(points, thresh=3.5):\n",
    "        if len(points.shape) == 1:\n",
    "            points = points[:, None]\n",
    "        median = np.median(points, axis=0)\n",
    "        diff = np.sum((points - median) ** 2, axis=-1)\n",
    "        diff = np.sqrt(diff)\n",
    "        med_abs_deviation = np.median(diff)\n",
    "\n",
    "        modified_z_score = 0.6745 * diff / med_abs_deviation\n",
    "\n",
    "        return modified_z_score > thresh\n",
    "\n",
    "    limits_dict = {}\n",
    "\n",
    "    df_numeric = df.select_dtypes(include=[np.number])\n",
    "    # cols = set(df_numeric.columns)\n",
    "    # cols = cols.difference(set(['time', 'lat', 'long', 'fix', 'speed', 'alt', 'head', 'rpm', 'id', 'ver',\n",
    "    #                             'index', 'vol', 'rtys']))\n",
    "    cols = ['afewrk1', 'afeaux1',  'afewrk2', 'afeaux2', 'afewrk3', 'afeaux3', 'afept1k', 'isbwrk','isbaux', 'pm1hum']\n",
    "    print(cols)\n",
    "    df_no_na = df.dropna(subset=cols)\n",
    "    print(df_no_na.shape,'df_no_na.shape')\n",
    "    id_list = df_no_na.id.unique()\n",
    "    id_list.sort()\n",
    "    for col in cols:\n",
    "        for device_id in id_list:\n",
    "#             print(df_no_na[df_no_na.id == device_id].shape)\n",
    "            df_device = df_no_na[df_no_na.id == device_id]\n",
    "            data_col_id = df_device[col].values\n",
    "            outliers = mad_based_outlier(data_col_id, thresh=thresh)\n",
    "#             print(outliers.shape)\n",
    "            n_entries = len(data_col_id)\n",
    "            perc_outliers = sum(outliers) / n_entries\n",
    "            if perc_outliers > 0.05:\n",
    "                print(\"Column: %s Device id %i Number of entries %i Share of outliers %f\" % (\n",
    "                    col, device_id, n_entries, perc_outliers))\n",
    "#             print(df_device.index.shape,'index.shape')\n",
    "#             print(outliers.shape,'outliers shape')\n",
    "#             print(df_no_na.shape,'df_no_na.shape')\n",
    "            df_no_na.loc[df_device.index, col + '_outlier'] = outliers\n",
    "#     print(df_no_na.columns)\n",
    "    df_clean = df_no_na.copy()\n",
    "    for col in cols:\n",
    "        outlier_col = col + '_outlier'\n",
    "        df_clean = df_clean[~df_clean[outlier_col]]\n",
    "    print(\"Total Number of entries %i Perc remaining: %f\" % (len(df_no_na), len(df_clean) / len(df_no_na)))\n",
    "    return df_clean[df.columns], limits_dict\n",
    "\n",
    "def remove_outliers_NO2_flexible(df, cols, thresh=3.5):\n",
    "    def mad_based_outlier(points, thresh=3.5):\n",
    "        if len(points.shape) == 1:\n",
    "            points = points[:, None]\n",
    "        median = np.median(points, axis=0)\n",
    "        diff = np.sum((points - median) ** 2, axis=-1)\n",
    "        diff = np.sqrt(diff)\n",
    "        med_abs_deviation = np.median(diff)\n",
    "\n",
    "        modified_z_score = 0.6745 * diff / med_abs_deviation\n",
    "\n",
    "        return modified_z_score > thresh\n",
    "\n",
    "    limits_dict = {}\n",
    "\n",
    "    df_numeric = df.select_dtypes(include=[np.number])\n",
    "    # cols = set(df_numeric.columns)\n",
    "    # cols = cols.difference(set(['time', 'lat', 'long', 'fix', 'speed', 'alt', 'head', 'rpm', 'id', 'ver',\n",
    "    #                             'index', 'vol', 'rtys']))\n",
    "    # cols = ['afewrk1', 'afeaux1',  'afewrk2', 'afeaux2', 'afewrk3', 'afeaux3', 'afept1k', 'isbwrk','isbaux', 'pm1hum']\n",
    "    print(cols)\n",
    "    df_no_na = df.dropna(subset=cols)\n",
    "    print(df_no_na.shape,'df_no_na.shape')\n",
    "    id_list = df_no_na.id.unique()\n",
    "    id_list.sort()\n",
    "    for col in cols:\n",
    "        for device_id in id_list:\n",
    "#             print(df_no_na[df_no_na.id == device_id].shape)\n",
    "            df_device = df_no_na[df_no_na.id == device_id]\n",
    "            data_col_id = df_device[col].values\n",
    "            outliers = mad_based_outlier(data_col_id, thresh=thresh)\n",
    "#             print(outliers.shape)\n",
    "            n_entries = len(data_col_id)\n",
    "            perc_outliers = sum(outliers) / n_entries\n",
    "            if perc_outliers > 0.05:\n",
    "                print(\"Column: %s Device id %i Number of entries %i Share of outliers %f\" % (\n",
    "                    col, device_id, n_entries, perc_outliers))\n",
    "#             print(df_device.index.shape,'index.shape')\n",
    "#             print(outliers.shape,'outliers shape')\n",
    "#             print(df_no_na.shape,'df_no_na.shape')\n",
    "            df_no_na.loc[df_device.index, col + '_outlier'] = outliers\n",
    "#     print(df_no_na.columns)\n",
    "    df_clean = df_no_na.copy()\n",
    "    for col in cols:\n",
    "        outlier_col = col + '_outlier'\n",
    "        df_clean = df_clean[~df_clean[outlier_col]]\n",
    "    print(\"Total Number of entries %i Perc remaining: %f\" % (len(df_no_na), len(df_clean) / len(df_no_na)))\n",
    "    return df_clean[df.columns], limits_dict\n",
    "\n",
    "\n",
    "def remove_outliers(df, thresh=3.5):\n",
    "    def mad_based_outlier(points, thresh=3.5):\n",
    "        if len(points.shape) == 1:\n",
    "            points = points[:, None]\n",
    "        median = np.median(points, axis=0)\n",
    "        diff = np.sum((points - median) ** 2, axis=-1)\n",
    "        diff = np.sqrt(diff)\n",
    "        med_abs_deviation = np.median(diff)\n",
    "\n",
    "        modified_z_score = 0.6745 * diff / med_abs_deviation\n",
    "\n",
    "        return modified_z_score > thresh\n",
    "\n",
    "    limits_dict = {}\n",
    "\n",
    "    df_numeric = df.select_dtypes(include=[np.number])\n",
    "    cols = set(df_numeric.columns)\n",
    "    cols = cols.difference(set(['time', 'lat', 'long', 'fix', 'speed', 'alt', 'head', 'rpm', 'id', 'ver',\n",
    "                                'index', 'vol', 'rtys']))\n",
    "    # cols = ['afewrk1', 'afeaux1',  'afewrk2', 'afeaux2', 'afewrk3', 'afeaux3', 'afept1k', 'isbwrk','isbaux', 'pm1hum']\n",
    "    print(cols)\n",
    "    df_no_na = df.dropna(subset=cols)\n",
    "    print(df_no_na.shape,'df_no_na.shape')\n",
    "    id_list = df_no_na.id.unique()\n",
    "    id_list.sort()\n",
    "    for col in cols:\n",
    "        for device_id in id_list:\n",
    "#             print(df_no_na[df_no_na.id == device_id].shape)\n",
    "            df_device = df_no_na[df_no_na.id == device_id]\n",
    "            data_col_id = df_device[col].values\n",
    "            outliers = mad_based_outlier(data_col_id, thresh=thresh)\n",
    "#             print(outliers.shape)\n",
    "            n_entries = len(data_col_id)\n",
    "            perc_outliers = sum(outliers) / n_entries\n",
    "            if perc_outliers > 0.05:\n",
    "                print(\"Column: %s Device id %i Number of entries %i Share of outliers %f\" % (\n",
    "                    col, device_id, n_entries, perc_outliers))\n",
    "#             print(df_device.index.shape,'index.shape')\n",
    "#             print(outliers.shape,'outliers shape')\n",
    "#             print(df_no_na.shape,'df_no_na.shape')\n",
    "            df_no_na.loc[df_device.index, col + '_outlier'] = outliers\n",
    "#     print(df_no_na.columns)\n",
    "    df_clean = df_no_na.copy()\n",
    "    for col in cols:\n",
    "        outlier_col = col + '_outlier'\n",
    "        df_clean = df_clean[~df_clean[outlier_col]]\n",
    "    print(\"Total Number of entries %i Perc remaining: %f\" % (len(df_no_na), len(df_clean) / len(df_no_na)))\n",
    "    return df_clean[df.columns], limits_dict\n",
    "\n",
    "\n",
    "def prepareData(data, test_size=0.2, drop_cols=[]):\n",
    "    data = pd.DataFrame(data.copy())\n",
    "\n",
    "    test_index = int(len(data) * (1 - test_size))\n",
    "\n",
    "    data = data.dropna()\n",
    "    data = data.reset_index(drop=True)\n",
    "\n",
    "    X_train = data.loc[:test_index].drop([\"y\"], axis=1)\n",
    "    y_train = data.loc[:test_index][\"y\"]\n",
    "    X_test = data.loc[test_index:].drop([\"y\"], axis=1)\n",
    "    y_test = data.loc[test_index:][\"y\"]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def shift(arr, num, fill_value=np.NaN):\n",
    "    result = np.empty_like(arr)\n",
    "    if num > 0:\n",
    "        result[:num] = fill_value\n",
    "        result[num:] = arr[:-num]\n",
    "    elif num < 0:\n",
    "        result[num:] = fill_value\n",
    "        result[:num] = arr[-num:]\n",
    "    else:\n",
    "        result[:] = arr\n",
    "    return result\n",
    "\n",
    "def add_alerts_to_np_array(value_col, alert_threshold=35,):\n",
    "    # weird numpy where this doesn't work for boolean arrays\n",
    "    this_hour_over_alert_threshold = (value_col > alert_threshold).astype(float)\n",
    "#     print(this_hour_over_alert_threshold)\n",
    "    hour_minus_one_over_alert_threshold = shift(this_hour_over_alert_threshold, 1, fill_value=np.NaN)\n",
    "#     print(shift(this_hour_over_alert_threshold, 1))\n",
    "#     print(hour_minus_one_over_alert_threshold)\n",
    "    hour_minus_two_over_alert_threshold = shift(this_hour_over_alert_threshold, 2, fill_value=np.NaN)\n",
    "#     print(hour_minus_two_over_alert_threshold)\n",
    "\n",
    "    alert_triggered = ((this_hour_over_alert_threshold + hour_minus_one_over_alert_threshold + hour_minus_two_over_alert_threshold) >= 2)\n",
    "    #df['alert_triggered'].astype(int).plot()\n",
    "    #print(sum(df.alert_triggered/len(df)))\n",
    "    return alert_triggered"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
